{
    "config.py->GlobalConfig": {
        // please checkout config.py for information
        "note": "example experiment",                   // in case you forget the purpose of this trainning session, write a note
        "env_name": "collective_assult",                // which environment, see ./MISSIONS/env_router.py
        "env_path": "MISSIONS.collective_assult",       // path of environment
        "draw_mode": "Img",                             // activate data plotting (Tensorboard is not used because I do not like it)
        "num_threads": "64",                            // run N parallel envs, a 'env' is refered to as a 'thread'
        "report_reward_interval": "64",                 // reporting interval
        "test_interval": "2048",                        // test every $test_interval episode
        "fold": "1",                                    // this 'folding' is designed for IPC efficiency, you can thank python GIL for such a strange design... 
        "seed": 8888,                                   // seed controls pytorch and numpy
        "backup_files": [                               // backup files, pack them up
            "example.jsonc",
            "ALGORITHM/conc",
            "MISSIONS/collective_assult/envs/collective_assult_env.py"
        ],
        "device": "cuda:0",                             // choose from 'cpu' (no GPU), 'cuda' (auto select GPU), 'cuda:3' (manual select GPU) 
        // GPU memory is precious! assign multiple training process to a 'party', then they will share GPU memory 
        "gpu_party": "off"                     // default is 'off', 
    },


    "MISSIONS.collective_assult.collective_assult_parallel_run.py->ScenarioConfig": {
        // please checkout ./MISSIONS/collective_assult/collective_assult_parallel_run.py for information
        "size": "5",
        "random_jam_prob": 0.1,
        "introduce_terrain": "True",
        "terrain_parameters": [
            0.05,
            0.2
        ],
        "num_steps": "180",
        "render": "False",
        "half_death_reward": "True",
        "TEAM_NAMES": [ //select team algorithm
            "ALGORITHM.conc.foundation->ReinforceAlgorithmFoundation"
        ]
    },
    "ALGORITHM.conc.foundation.py->AlgorithmConfig": {
        // please checkout ./ALGORITHM/conc/foundation.py for information
        "n_focus_on": 4,                    // d_k
        "actor_attn_mod": "False",          // add self-attention to actor network
        "lr": 0.0005,
        "ppo_epoch": 24,
        "train_traj_needed": "64",
        "load_checkpoint": false
    }
}